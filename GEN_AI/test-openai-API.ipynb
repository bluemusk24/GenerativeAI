{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ce5f84-073d-412b-bcfe-06c6398c6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d0e8f-345a-4cb7-97af-8230373e2056",
   "metadata": {},
   "source": [
    "##### Generate OPENAI API KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f51123-e407-4ac7-a1ad-a393dec9001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_openai_key = '*********'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b651592-eb37-4fce-8852-089fb305e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the openaikey\n",
    "openai.api_key = my_openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3c3dc0-ef3a-4c83-8ee7-908e7735ee86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'),\n",
       " Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'),\n",
       " Model(id='davinci-002', created=1692634301, object='model', owned_by='system'),\n",
       " Model(id='babbage-002', created=1692634615, object='model', owned_by='system'),\n",
       " Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'),\n",
       " Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'),\n",
       " Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'),\n",
       " Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'),\n",
       " Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'),\n",
       " Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-0125', created=1706048358, object='model', owned_by='system'),\n",
       " Model(id='text-embedding-3-large', created=1705953180, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'),\n",
       " Model(id='text-embedding-3-small', created=1705948997, object='model', owned_by='system'),\n",
       " Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai'),\n",
       " Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'),\n",
       " Model(id='gpt-3.5-turbo-16k-0613', created=1685474247, object='model', owned_by='openai')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check all openai models\n",
    "all_openai_models = openai.models.list()\n",
    "list(all_openai_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb23222-aa28-4b03-9d31-32fcef303c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08fb477a-6c0a-42a0-a3b2-ed5dd9c82b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>created</th>\n",
       "      <th>object</th>\n",
       "      <th>owned_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(id, dall-e-3)</td>\n",
       "      <td>(created, 1698785189)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(id, whisper-1)</td>\n",
       "      <td>(created, 1677532384)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(id, davinci-002)</td>\n",
       "      <td>(created, 1692634301)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(id, babbage-002)</td>\n",
       "      <td>(created, 1692634615)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(id, dall-e-2)</td>\n",
       "      <td>(created, 1698798177)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(id, gpt-3.5-turbo-16k)</td>\n",
       "      <td>(created, 1683758102)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(id, tts-1-hd-1106)</td>\n",
       "      <td>(created, 1699053533)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(id, tts-1-hd)</td>\n",
       "      <td>(created, 1699046015)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(id, gpt-3.5-turbo-1106)</td>\n",
       "      <td>(created, 1698959748)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(id, gpt-3.5-turbo-instruct-0914)</td>\n",
       "      <td>(created, 1694122472)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(id, gpt-3.5-turbo-instruct)</td>\n",
       "      <td>(created, 1692901427)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(id, tts-1)</td>\n",
       "      <td>(created, 1681940951)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(id, gpt-3.5-turbo-0301)</td>\n",
       "      <td>(created, 1677649963)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(id, tts-1-1106)</td>\n",
       "      <td>(created, 1699053241)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(id, gpt-3.5-turbo-0125)</td>\n",
       "      <td>(created, 1706048358)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(id, text-embedding-3-large)</td>\n",
       "      <td>(created, 1705953180)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(id, gpt-3.5-turbo)</td>\n",
       "      <td>(created, 1677610602)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(id, text-embedding-3-small)</td>\n",
       "      <td>(created, 1705948997)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, system)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(id, gpt-3.5-turbo-0613)</td>\n",
       "      <td>(created, 1686587434)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(id, text-embedding-ada-002)</td>\n",
       "      <td>(created, 1671217299)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai-internal)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(id, gpt-3.5-turbo-16k-0613)</td>\n",
       "      <td>(created, 1685474247)</td>\n",
       "      <td>(object, model)</td>\n",
       "      <td>(owned_by, openai)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model_id                created           object  \\\n",
       "0                      (id, dall-e-3)  (created, 1698785189)  (object, model)   \n",
       "1                     (id, whisper-1)  (created, 1677532384)  (object, model)   \n",
       "2                   (id, davinci-002)  (created, 1692634301)  (object, model)   \n",
       "3                   (id, babbage-002)  (created, 1692634615)  (object, model)   \n",
       "4                      (id, dall-e-2)  (created, 1698798177)  (object, model)   \n",
       "5             (id, gpt-3.5-turbo-16k)  (created, 1683758102)  (object, model)   \n",
       "6                 (id, tts-1-hd-1106)  (created, 1699053533)  (object, model)   \n",
       "7                      (id, tts-1-hd)  (created, 1699046015)  (object, model)   \n",
       "8            (id, gpt-3.5-turbo-1106)  (created, 1698959748)  (object, model)   \n",
       "9   (id, gpt-3.5-turbo-instruct-0914)  (created, 1694122472)  (object, model)   \n",
       "10       (id, gpt-3.5-turbo-instruct)  (created, 1692901427)  (object, model)   \n",
       "11                        (id, tts-1)  (created, 1681940951)  (object, model)   \n",
       "12           (id, gpt-3.5-turbo-0301)  (created, 1677649963)  (object, model)   \n",
       "13                   (id, tts-1-1106)  (created, 1699053241)  (object, model)   \n",
       "14           (id, gpt-3.5-turbo-0125)  (created, 1706048358)  (object, model)   \n",
       "15       (id, text-embedding-3-large)  (created, 1705953180)  (object, model)   \n",
       "16                (id, gpt-3.5-turbo)  (created, 1677610602)  (object, model)   \n",
       "17       (id, text-embedding-3-small)  (created, 1705948997)  (object, model)   \n",
       "18           (id, gpt-3.5-turbo-0613)  (created, 1686587434)  (object, model)   \n",
       "19       (id, text-embedding-ada-002)  (created, 1671217299)  (object, model)   \n",
       "20       (id, gpt-3.5-turbo-16k-0613)  (created, 1685474247)  (object, model)   \n",
       "\n",
       "                       owned_by  \n",
       "0            (owned_by, system)  \n",
       "1   (owned_by, openai-internal)  \n",
       "2            (owned_by, system)  \n",
       "3            (owned_by, system)  \n",
       "4            (owned_by, system)  \n",
       "5   (owned_by, openai-internal)  \n",
       "6            (owned_by, system)  \n",
       "7            (owned_by, system)  \n",
       "8            (owned_by, system)  \n",
       "9            (owned_by, system)  \n",
       "10           (owned_by, system)  \n",
       "11  (owned_by, openai-internal)  \n",
       "12           (owned_by, openai)  \n",
       "13           (owned_by, system)  \n",
       "14           (owned_by, system)  \n",
       "15           (owned_by, system)  \n",
       "16           (owned_by, openai)  \n",
       "17           (owned_by, system)  \n",
       "18           (owned_by, openai)  \n",
       "19  (owned_by, openai-internal)  \n",
       "20           (owned_by, openai)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert all openai models from list to Dataframe\n",
    "openai_df = pd.DataFrame(list(all_openai_models), columns=['model_id', 'created', 'object', 'owned_by'])\n",
    "openai_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b1cc9-ea6d-4f7f-909b-ed56992b1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai codes from openai playground. Chat completion API and function calling.\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=my_openai_key)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo-16k\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"You are a helpful assistant\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"how can I make money online\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  temperature=1,\n",
    "  max_tokens=256,\n",
    "  top_p=1,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  n = number_of_outputs\n",
    ")\n",
    "\n",
    "# response.choices[0].message.content : how to call the response above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b00d0d-8a21-4c50-8080-a3a6410ba9bf",
   "metadata": {},
   "source": [
    "###### RAG - Retrieval Augmented Generation is AI framework that retrieves data from external sources of knowledge to improve the quality of responses. RAG helps responses to be more accurate and up-to-date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b470fcd-da59-4cc4-9eec-efe0acd57b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emmanuel is a mechanical engineering graduate from South Plains college. He is originally from Nigeria but resides in Lubbock,Texas.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_description = 'Emmanuel is a mechanical engineering graduate from South Plains college. He is originally from Nigeria but resides in Lubbock,Texas.'\n",
    "student_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265510f1-b0b4-4b00-aa62-a25282e6dc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple prompt to extract information from 'student_descripton' in JSON format\n",
    "prompt = f'''\n",
    "Please extract the following information from a given text and return it as a JSON object:\n",
    "\n",
    "name\n",
    "college\n",
    "country\n",
    "residence\n",
    "\n",
    "This is the body of text to extract the information from:\n",
    "{student_description}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "757b7bdb-6380-4f9e-99c1-1807bc5ae8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please extract the following information from a given text and return it as a JSON object:\n",
      "\n",
      "name\n",
      "college\n",
      "country\n",
      "residence\n",
      "\n",
      "This is the body of text to extract the information from:\n",
      "Emmanuel is a mechanical engineering graduate from South Plains college. He is originally from Nigeria but resides in Lubbock,Texas.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run the prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea307e0d-76cb-4407-808d-aa6901ab627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the prompt to openai\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=my_openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb21dee-05f3-40e4-a166-0eba6d9bee3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x77dbcbc51cf0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a2682d-ffb4-4e0a-adeb-0110b6f6ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call chat completion api\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# output = response.choices[0].message.content gave an error because of pricing. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ee124-8843-4aa0-8086-23820ebe7161",
   "metadata": {},
   "source": [
    "##### getting output in JSon format\n",
    "##### import json\n",
    "##### json.loads(output) --> this should give the extracted information of the prompt in json format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893f202-ab38-4dd7-a443-e60c7aa89141",
   "metadata": {},
   "source": [
    "### LANGCHAIN:\n",
    "##### this is a wrapper that gives access to different APIs(Openai, hugging-face models), agents, prompt template, document loaders, chains, data-sources(google, wikipedia)\n",
    "##### Prompts are sent to langchain and then to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dbfe544-040f-4821-b078-76c23d5e48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a68dc65f-8114-4601-a260-2a877db7b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access OpenAI via langchain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc1db0-cf1a-4386-95fc-a9da17f545f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator object of OpenAI\n",
    "client = OpenAI(api_key=my_openai_key)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb1845d-208b-410b-ac55-a3bb3f457fc9",
   "metadata": {},
   "source": [
    "##### CASE-STUDY 1 of LANGCHAIN: Zero-shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03532d16-0cd4-43fa-8514-d9013d753f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your zero-shot prompt\n",
    "prompt = 'tell me the best cities in the world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bdeb47-0645-4913-a0a0-ba8b90f4977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying prompt to OpenAI via langchain. This gave an error bcos of billing and prices(limitations of OpenAI)\n",
    "output = print(client.predict(prompt).strip())\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06475ebd-ccc6-40bb-bd33-34b2c4fe0374",
   "metadata": {},
   "source": [
    "##### CASE-STUDY 2 of LANGCHAIN: Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "352b6a64-1d71-4c29-861e-fdbec3799578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import prompt-template class\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03ff9b4b-1dfa-4ebf-8964-8f2a25f79efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a google map and a current affairs expert. Please tell me what is the capital of Nigeria?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimator object of the PromptTemplate class with some parameters\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template='You are a google map and a current affairs expert. Please tell me what is the capital of {country}?'\n",
    ")\n",
    "\n",
    "# call a method off prompt_template\n",
    "prompt1 = prompt_template.format(country='Nigeria')\n",
    "prompt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f6413c3-1c98-446b-8e04-cd7bb1a49812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a google map and a current affairs expert. Please tell me what is the capital of USA?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call a method off prompt_template\n",
    "prompt2 = prompt_template.format(country='USA')\n",
    "prompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ac695-a4bf-4818-8a73-0c179039689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying prompt1 and prompt2 to OpenAI gpt model via langchain. This gave an error bcos of billing and prices(limitations of OpenAI\n",
    "output_1 = print(client.predict(prompt1).strip())\n",
    "output_2 = print(client.predict(prompt2).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc14cf21-8bef-41b4-b360-8673ae1ce3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is the capital of England'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second way of using PromptTemplate\n",
    "prompt3 = PromptTemplate.from_template('what is the capital of {country}')\n",
    "prompt3 = prompt3.format(country='England')\n",
    "prompt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9801109-6272-430b-979a-db90c3693593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying prompt3 to OpenAI gpt model via langchain. This gave an error bcos of billing and prices(limitations of OpenAI\n",
    "output_3 = print(client.predict(prompt3).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6072c9-a080-4bdb-a0e3-3b445a0539af",
   "metadata": {},
   "source": [
    "##### CASE-STUDY 3 of LANGCHAIN: AGENTS\n",
    "\n",
    "##### Agents gives up-to-date information extracted from external data sources (third-party tool). This happens when the gpt-model, trained on old data is not current with latest data\n",
    "\n",
    "##### we use serp API for extracting real-time information from google-search engine. pip install google-search-results here or in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb4dfa1f-12b3-4fa9-af76-33bd3914ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-search-results !pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c325088-b4cd-4c89-b9a6-0d910601826e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serp api_key from serp api.com\n",
    "serp_api_key = '***********'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1e5b152-e580-4a89-990b-cc378bdfac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import different agents from langchain\n",
    "from langchain.agents import AgentType, load_tools, initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951861e-973c-47a1-aa8a-4ebb6430ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimaor object of OpenAI key with client.\n",
    "client = OpenAI(api_key=my_openai_key)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7191608f-d7bc-4085-bcc1-327cf82f7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the agent tools with serpAPI and client OpenAPI gpt-model\n",
    "tool = load_tools(['serpapi'], serpapi_api_key=serp_api_key, llm=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35360671-1b12-4ea6-9449-c1830791bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/envs/gen-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# get the esimator object of agent type\n",
    "agent = initialize_agent(tool, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2473e1-ce76-48e9-a01d-78b0886f166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the agent with an input prompt. this should give an error bcos of billing and prices\n",
    "agent.run('tell me who won the last FIFA world cup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f39140ae-101b-4fe0-988f-a5c8a4fbe755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tool from wikipedia \n",
    "tool = load_tools(['wikipedia'], llm=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98361f58-aa8d-429b-813c-c175a28ed254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the esimator object of agent type and run the agent\n",
    "agent = initialize_agent(tool, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run('how can i get rich')     # error bcos billing and price. info taken from wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125948b-c511-4aff-a223-859e4ece6514",
   "metadata": {},
   "source": [
    "#### CHAINS : \n",
    "##### They form the connection among one or several LLMs. In complex applications, it's necessary to chain LLMs either together or with other elements. eg. connecting an agent (serp_API) with LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d959e726-286e-44dc-be84-d491224399d6",
   "metadata": {},
   "source": [
    "###### CHAINS-CASE-STUDY_1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b20957b-2db7-48f5-97ee-db2716f2bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call openai gpt-model\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74a19be4-5365-4890-be71-488a69645542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], template='who is the football best player in {country}')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define prompts\n",
    "prompt = PromptTemplate.from_template('who is the football best player in {country}')\n",
    "#prompt = prompt.format(country='England')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51e89766-ef75-4675-a314-0d760c4c4c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['product'], template='give a good company that makes this {product}?')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define prompt_1\n",
    "prompt_1 = PromptTemplate.from_template('give a good company that makes this {product}?')\n",
    "#prompt_1 = prompt_1.format(product='Wine')\n",
    "prompt_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52ceab8c-bc32-4562-86df-4e027f25ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LLM chain\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e21898-0f8c-4da4-8602-20e2e5cb0ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the chain estimator object with paramters llm and Prompt.Template\n",
    "chain = LLMChain(llm=client, prompt=prompt)\n",
    "chain.run('England').strip()         # error bcos of billing and prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094a095-bc2b-4e0a-8c62-788cc552022d",
   "metadata": {},
   "source": [
    "###### CHAINS-CASE-STUDY_2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f379c251-60c3-4b80-a987-8af9e92385b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['cuisine'], template='I want to open a restaurant for {cuisine} food. Suggest a fancy name for this')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another format of prompt template\n",
    "prompt_temp = PromptTemplate(\n",
    "    input_variables= ['cuisine'],\n",
    "    template = 'I want to open a restaurant for {cuisine} food. Suggest a fancy name for this'\n",
    ")\n",
    "\n",
    "prompt_temp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1b902-fc1e-4f01-8f4b-53ae9ada6a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain the client LLM and prompts\n",
    "chain = LLMChain(llm=client, prompt=prompt_temp, verbose=True)       # verbose for complete detail of the output prompt\n",
    "chain.run('Nigerian').strip()     # error bcos of billing and prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1305cb69-6cee-4039-af78-45fe4cc0a8ca",
   "metadata": {},
   "source": [
    "##### Simple Sequential Chain : creating a sequence to combine 2 different chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af3ef6e0-4770-4e6a-a69b-de4a01499d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt and chain 1\n",
    "prompt_temp_1 = PromptTemplate(\n",
    "    input_variables= ['cuisine'],\n",
    "    template = 'I want to open a restaurant for {cuisine} food. Suggest a fancy name for this'\n",
    ")\n",
    "first_chain = LLMChain(llm=client, prompt=prompt_temp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b55bd08c-4054-45b5-86a4-ce05b98efed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt and chain 2\n",
    "prompt_temp_2 = PromptTemplate(\n",
    "    input_variables= ['name'],\n",
    "    template = 'Tell me how to make the {name} of the food'\n",
    ")\n",
    "\n",
    "second_chain = LLMChain(llm=client, prompt=prompt_temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8435c1-fa3e-4bd6-a2ac-c1ae4f6dc582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import simple sequential chain and include both chains above as parameters\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "combined_chain = SimpleSequentialChain(chains=[first_chain, second_chain])\n",
    "print(combined_chain.run('Nigerian'))      # errors of billing and price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558c888-1554-47bd-ae58-c9c7b8e78689",
   "metadata": {},
   "source": [
    "##### Sequential Chain : more powerful than Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f4aeb42-4796-4726-b5dc-f698420cb892",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt and chain 1 with and output key\n",
    "prompt_temp_1 = PromptTemplate(\n",
    "    input_variables= ['cuisine'],\n",
    "    template = 'I want to open a restaurant for {cuisine} food. Suggest a fancy name for this'\n",
    ")\n",
    "first_chain = LLMChain(llm=client, prompt=prompt_temp_1, output_key='restaurant name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b50f7575-42fc-4c18-b503-f528978dceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt and chain 2 with input variables as the output key of the above prompt. \n",
    "prompt_temp_2 = PromptTemplate(\n",
    "    input_variables= ['restaurant name'],\n",
    "    template = 'suggest some menu items for {restaurant name}'\n",
    ")\n",
    "\n",
    "second_chain = LLMChain(llm=client, prompt=prompt_temp_2, output_key='menu_items')   # menu_itmes will be the input variable of the next prompt if created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19d69b41-66bf-4cb7-89fe-ed6b072d6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sequential chain and include both chains above as parameters\n",
    "from langchain.chains import SequentialChain\n",
    "combined_chain = SequentialChain(\n",
    "    chains=[first_chain, second_chain],\n",
    "    input_variables=['cuisine'],\n",
    "    output_variables=['restaurant name', 'menu_items']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dbd852-1c2f-4da9-930a-c5f30d68d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run chains. errors bcos pricng and billing\n",
    "combined_chain({'cuisine': 'Nigerian'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97dc60d-da3f-4bea-9690-efd4ab6a200d",
   "metadata": {},
   "source": [
    "#### Document Loader with Langchain:\n",
    "\n",
    "##### when you want to load a document into the gpt model. eg csv, pdf, html, json. google document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a65b4af-e358-4944-9893-63426e311af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /home/codespace/.local/lib/python3.10/site-packages (from pypdf) (4.10.0)\n",
      "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-4.2.0\n"
     ]
    }
   ],
   "source": [
    "# install pdf from document loader documentation\n",
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab984068-6c6e-4eb3-acc4-d86b3eac4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this import was from langchain document loader documentation\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a82b9a1f-f15b-4fff-bac0-48571e09388c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x77dbc942c850>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instantiate the PyPDFLoader and pass any PDF document to read.\n",
    "loader = PyPDFLoader(\"Machine Learning Notes/Bias Variance Trade-Off.pdf\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c88a2303-d82b-4960-a646-b7a597631acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n",
      "could not convert string to float: '0.00-50' : FloatObject (b'0.00-50') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Model Evaluation', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 0}), Document(page_content='Model Evaluation is a fundamental topic of \\nunderstanding your model’s performance! \\nReview Chapter 2 of Introduction to Statistical \\nLearning  for a more in depth look! \\nCompanion Book', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 1}), Document(page_content='Model Evaluation \\n●We’ll discuss the following topics \\n○Train Test Splits \\n○Holdout Sets \\n○Parameter Grids \\n○Scala and Spark for Model Evaluation \\n○Bias Variance Trade-Off \\n○Documentation Exploration  \\n○Code through some Examples', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 2}), Document(page_content='Train Test Splits \\n●We’ve previously talked about Train Test Splits, but let’s  \\nreview the concept. \\n●You will always train a Machine Learning Algorithm on  \\nsome data, but afterwards you will want some measure of  \\nhow well it performed. \\n●Each main Machine Learning Task has different metrics  \\nfor evaluation', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 3}), Document(page_content='Train Test Splits \\n●Regression \\n○R^2\\n○RMSE \\n●Classification \\n○Precision \\n○Recall \\n●Clustering \\n○Within Sum of Squares Error', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 4}), Document(page_content='Train Test Splits \\n●While you could get these measurements using the same  \\ndata you trained your model on, that is not a good idea. \\n●Your model has already seen this data meaning it is not a  \\ngood choice for evaluating your model’s performance \\n●You should get these metrics off test data, which your  \\nmodel has not seen yet. \\n●This is known as a train-test split.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 5}), Document(page_content='Holdout Data \\n●An expansion of this idea is the holdout data set. \\n●This is separate from the training and test sets. \\n●In this process you use the training data to fit your model,  \\nyou use the test set to evaluate and adjust your model. \\n●You can use the test set over and over again. \\n●Finally, before deploying your model, you check it against  \\nthe holdout to get some final metrics on performance.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 6}), Document(page_content='Parameter Grids \\n●As we’ve seen, we can add optional parameters to  \\nMachine Learning Algorithms. \\n●Many times it is difficult to know what are good values for  \\nthese parameters. \\n●Spark makes it possible to set up a grid of parameters to  \\ntrain across. \\n●You create multiple models, train them across the grid,  \\nand Spark reports back which model performed best.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 7}), Document(page_content='Spark and Scala for Model Evaluation \\n●Spark makes all of these processes generally easy with  \\nthe use of 3 object types: \\n○Evaluators \\n○ParamGridBuilders \\n○TrainValidationSplit \\n●Later on in this section we will explore how to use these  \\nobject types to implement the ideas discussed here.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 8}), Document(page_content='Spark and Scala for Model Evaluation \\n●An important aspect to understanding all of this is the  \\nBias-Variance Trade-Off. \\n●We won’t directly explore this with Spark and Scala  \\nbecause it pertains more to theory than Data Engineering. \\n●However let’s take the time now to at least understand the  \\nconcept so we can have full context for this section of the  \\ncourse.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 9}), Document(page_content='Bias Variance Trade-Off \\n●The bias-variance trade-off is the point where we are  \\nadding just noise by adding model complexity (flexibility).  \\n●The training error goes down as it has to, but the test error  \\nis starting to go up.  \\n●The model after the bias trade-off begins to overfit.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 10}), Document(page_content='Bias Variance Trade-Off \\n●Imagine that the center of the  \\ntarget is a model that perfectly  \\npredicts the correct values.  \\n●As we move away from the  \\nbulls-eye, our predictions get  \\nworse and worse.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 11}), Document(page_content='Bias Variance Trade-Off \\n●Imagine we can repeat our  \\nentire model building process  \\nto get a number of separate hits  \\non the target.  \\n●Each hit represents an  \\nindividual realization of our  \\nmodel, given the chance  \\nvariability in the training data  \\nwe gather.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 12}), Document(page_content='Bias Variance Trade-Off \\n●Sometimes we will get a good  \\ndistribution of training data so  \\nwe predict very well and we are  \\nclose to the bulls-eye, while  \\nsometimes our training data  \\nmight be full of outliers or  \\nnon-standard values resulting in  \\npoorer predictions.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 13}), Document(page_content='Bias Variance Trade-Off \\n●These different realizations  \\nresult in a scatter of hits on the  \\ntarget.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 14}), Document(page_content='Bias Variance Trade-Off \\n●A common temptation for beginners is to continually add \\ncomplexity to a model until it fits the training set very well.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 15}), Document(page_content='Bias Variance Trade-Off \\n●Doing this can cause a model to overfit to your training \\ndata and cause large errors on new data, such as the test \\nset.\\n●Let’s take a look at an example model on how we can see \\noverfitting occur from a error standpoint using test data! \\n●We’ll use a black curve with some “noise” points off of it to \\nrepresent the True shape the data follows.', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 16}), Document(page_content='Bias Variance Trade-Off \\n●', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 17}), Document(page_content='Bias Variance Trade-Off', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 18}), Document(page_content='Let’s continue by going \\nthrough some examples and \\nexploring the documentation!', metadata={'source': 'Machine Learning Notes/Bias Variance Trade-Off.pdf', 'page': 19})]\n"
     ]
    }
   ],
   "source": [
    "# get the pages of the pdf document\n",
    "pages = loader.load_and_split()\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c515a-94b8-4e7f-9a77-2b385c07a935",
   "metadata": {},
   "source": [
    "##### Note: check document loader langchain documentation for information on how to read any file (csv, html, json, markdown etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79342591-6924-423f-a96e-91e437bbf293",
   "metadata": {},
   "source": [
    "#### Langchain Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69811ae-1c6d-4d08-9dd0-b1c6a2a46653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "adbd6cdc-051b-4366-a75c-4343d9a8683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import agent_types, load_tools, initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a21e5ce5-fd80-4a07-8e04-2803944951a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=my_openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d1c00f0-0679-4c52-a573-9c5f1a77d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt template\n",
    "prompt_temp = PromptTemplate(\n",
    "    input_variables= ['product'],\n",
    "    template = 'What is a good name of a company that makes {product}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d99443-40a6-42ee-94f6-fee7640d4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LLm chain\n",
    "chain = LLMChain(llm=client, prompt=prompt_temp)\n",
    "print(chain.run('Bag').strip())       # this should error bcos of billing and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58c68f2f-ebb8-4fab-bddf-7c6d12ef01d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create another prompt template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables= ['product'],\n",
    "    template = 'What is a good name of a company that makes {product}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d87de-c731-4266-814c-b568a07c4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LLm chain\n",
    "chain = LLMChain(llm=client, prompt=prompt)\n",
    "print(chain.run('drone').strip())       # this should error bcos of billing and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c30c45b5-7b2d-4f55-bb99-7d144b34dde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling chain memory methods\n",
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7060cc1-88ac-4b2a-83ed-8cd63f21df75",
   "metadata": {},
   "source": [
    "##### Conversation Buffer Memory : to store the memory. These memory codes gave error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3afc561-e2aa-409e-a257-531c00f39aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3eb9ab1-e50e-4c00-ad1e-022599046640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.memory.buffer.ConversationBufferMemory"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimator object \n",
    "memory = ConversationBufferMemory\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f5440a9-4628-4e0d-b083-3635f75f1445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a prompt template\n",
    "prompt_temp = PromptTemplate(\n",
    "    input_variables= ['product'],\n",
    "    template = 'What is a good name of a company that makes {product}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7349940-d95c-4c7e-90e2-c5554cec3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the memomry parameter into the lLMChain\n",
    "chain = LLMChain(llm=client, prompt=prompt_temp, memory=memory)\n",
    "chain.run('Football')            # this should error bcos of billing and price. Also got another error\n",
    "chain.run('Wine')\n",
    "chain.run('Cookies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a671a11f-eb5b-411c-b1c3-948e830c7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling chain memory methods. do this after the above errors\n",
    "chain.memory\n",
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a2ce6-cd2b-4f89-8eb9-b8261f8db447",
   "metadata": {},
   "source": [
    "##### Conversation Chain : remembers last 5 conversation chain/ last 10-20 conversation chain. This works like chatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "210f6cd3-0e1a-4718-ad87-b26efcac57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08f05eb7-98d3-44c7-86b5-6793a6f5157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator object\n",
    "convo = ConversationChain(llm=OpenAI(api_key=my_openai_key, temperature=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f96d79b-248e-40d3-8d26-f5a94401f16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call a method on convo object\n",
    "convo.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b93b68d6-fa46-4d04-9564-afc4620d3dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e5129-f22b-4d20-a1d1-b78542234316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask the convo a question---- error of billing and price\n",
    "convo.run('who is the richest man in the world')\n",
    "\n",
    "# convo.run('any question') --> this conversation can be sustained by the gpt model in a chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68efa2fd-e356-4de4-a949-72c9522aeb15",
   "metadata": {},
   "source": [
    "##### Conversation Buffer Windows Memory : this invovles applying a parameter key(k) to determine how many input prompt it should sustain. k can be any value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62805363-3ba7-4ae7-a2f4-ea35b1d40442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc62eadd-3973-41a6-a02f-ccfc9a27a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)       # key(k) sustains just one input prompt. any other prompt is not sustained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6153dfff-0ad7-47f2-b7fa-d716e60bcc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo = ConversationChain(llm=OpenAI(api_key=my_openai_key, temperature=0.7), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d4321-0537-40ed-87bb-9b024f1bd3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask the convo a question---- error of billing and price\n",
    "convo.run('who is the richest man in the world')\n",
    "\n",
    "# convo.run('any question') --> this conversation can be sustained by the gpt model in a chain, given the value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f1610-c68a-49e3-b9e4-edc23d42722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### read the langchain documentation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
