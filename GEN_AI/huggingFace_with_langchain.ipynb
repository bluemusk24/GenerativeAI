{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cb0a47c-2f41-470c-8d3a-a7d6428db345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7da235-b165-4cd4-a7cb-718eb148ac7a",
   "metadata": {},
   "source": [
    "##### set the environment variable with token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82c4d01-7a49-4192-94ef-3e15dded506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '**************'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75cf8af-799d-4ac8-9620-5a49c8d16f91",
   "metadata": {},
   "source": [
    "##### Text2Text | Seq2Seq | Encoder-Decoder Models : on hugging face, click text2text model and select google/flan-t5-base or any listed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af167e0-5a44-4a4b-85d0-f0264852e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['product'],\n",
    "    template='You are my assitant. Suggest to me a manufacturer of this {product}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb5266d-cd4c-4843-be27-377afacfb311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/envs/gen-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.3.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n",
      "/home/codespace/anaconda3/envs/gen-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "#instantiate LLM object with included paramters\n",
    "chain = LLMChain(llm=HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={'temperature':0}), prompt=prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2565b7cc-bad2-4aaa-9ea2-de9b40dd7677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/envs/gen-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('samsung', 'samsung')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the chain\n",
    "chain.run('Wine'), chain.run('football')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51a69e21-7582-4810-9c2a-89ef37614eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sony'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('camera')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99bc50bf-52c7-4817-8b0b-1bbe35124548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the LLM chain with google-flan-t5-base hugging-face model\n",
    "chain1 = LLMChain(llm=HuggingFaceHub(repo_id='google/flan-t5-base', model_kwargs={'temperature':0}), prompt=prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b0462a-dc43-48ca-93e0-c89d024bd73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('honda', 'fsa', 'hms')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1.run('cars'), chain1.run('aeroplanes'), chain1.run('wristwatches')     # different types of products with chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd79fbf-e191-4e0a-9934-c4bd1ade519e",
   "metadata": {},
   "source": [
    "##### Using another hugging-face model - facebook/blenderbot-400M-distill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15aaa9cd-7a10-4be8-b42f-7c383b48c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template='You are my assitant. Suggest to me the best fottballer in this {country}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96d0f5b7-6679-4e93-997f-8a5034a2655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate LLM object with included paramters\n",
    "chain = LLMChain(llm=HuggingFaceHub(repo_id='facebook/blenderbot-400M-distill', model_kwargs={'temperature':1.5}), prompt=prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eddd7b6c-3a5e-4cfe-b5a3-e5bcb6edf3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't know much about football, but I do know that the game is played on a rectangular field.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the chain with facebook/blenderbot-400M-distill model\n",
    "chain.run('England')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06211db8-eee9-4648-9a27-8a7c8921e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the LLM chain with google-flan-t5-base hugging-face model\n",
    "chain1 = LLMChain(llm=HuggingFaceHub(repo_id='google/flan-t5-base', model_kwargs={'temperature':2}), prompt=prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4df48976-a563-4150-9dab-0a8c0286d496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't know much about football, but I do know that Spain has the best team in the world.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the chain with google/flan-t5-base\n",
    "chain.run('Spain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ad91ca-3fc1-4513-98d9-944f992b2dc5",
   "metadata": {},
   "source": [
    "#### Text Generation Models | Decoder Only Models... HuggingFace Pipeline with downloaded model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b36dc1f-8aaa-4757-b50c-bf8a816317a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e88f18dd-844c-4e51-8ab4-14aee829eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model from hugging face\n",
    "model_id = 'google/flan-t5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4489e66b-af65-4cfc-a7f1-42fd59fda1ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8909decaf14331aa928fe6cc3db47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab0d4d15dc345a6b839e5e8c3785b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467b0c6c7035463582588139d299c921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ed0e09adcf4cbf9bf27bec5ddd3486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# estimator object of AutoTokenizer with a method\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7b66fe8-8c03-4a50-b78d-6b03bc3ef020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fef9b7585e49ecad362ecd9fa4a2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c43102f2b0042749c0b7fa0af073588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b33f541be34edf89c8272b500bafcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# estimatir object of AutoModelForSeq2Seq\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1102266d-4a0b-4500-b23e-413febf05b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pipeline with parameters task=text2text-generation, model, tokenizer)\n",
    "pipeline = pipeline('text2text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d684c8-0951-428d-9d69-c289aca6dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/envs/gen-env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Hugging face pipeline with pipeline above\n",
    "local_llm = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8054e15-b663-4b04-b18b-1291ac6ad095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a prompt\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['name'],\n",
    "    template='You are my assitant. Tell me about footballer {name}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93f6a506-cca6-4757-8b4c-5748910e2984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/envs/gen-env/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Messi is a footballer who plays as a midfielder for Barcelona.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling chain\n",
    "chain = LLMChain(llm=local_llm, prompt=prompt)\n",
    "chain.run('Messi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "97528189-bb7e-4339-bef6-683094a801b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/envs/gen-env/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Okocha is a Nigerian footballer who played as a midfielder.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('Okocha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ea50845-ca26-4a3f-a3be-5c6750bc080f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/envs/gen-env/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Neymar is a Brazilian footballer who plays for Real Madrid.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('Neymar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09c0e017-d95a-45ae-8112-ed85aec4fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "    input_variables=['country'],\n",
    "    template='You are my assitant. tell me the name of this {country} President'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c8131fb2-9de0-445b-a54e-ca57404ef63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling chain\n",
    "chain = LLMChain(llm=local_llm, prompt=prompt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aed94978-dfff-4fe1-a57b-502664ea4f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/anaconda3/envs/gen-env/lib/python3.10/site-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'john mccain'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling chain\n",
    "chain.run('USA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71f4bb-3f9c-4671-b295-f247d1472559",
   "metadata": {},
   "source": [
    "##### downloading another LLM model 'google/flan-t5-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeaa659-b978-4164-a574-7d44b2e25774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model id of Facebook Meta llama from hugging face API\n",
    "model_id = 'meta-llama/Meta-Llama-3-8B'\n",
    "# estimator object of AutoTokenizer with a method\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# estimatir object of AutoModelForSeq2Seq\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f36c1e-8616-4076-9dcd-18e89b9bc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pipeline with parameters task=text2text-generation, model, tokenizer)\n",
    "pipeline_1 = pipeline('text2text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8922df-e792-4c3e-90c9-36745e7bed84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
